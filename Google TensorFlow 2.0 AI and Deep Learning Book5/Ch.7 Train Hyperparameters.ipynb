{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from preprocessing import parse_aug_fn, parse_fn\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_list = [False, True]\n",
    "bn_list = [False, True]\n",
    "init_list = ['RanodmNormal_0.01std', 'glorot_normal', 'he_normal']\n",
    "lr_list = [0.001, 0.01, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_ia = hp.HParam(name='ia', domain=hp.Discrete(ia_list), display_name='Image Augmentation')\n",
    "\n",
    "hp_bn = hp.HParam(name='bn', domain=hp.Discrete(bn_list), display_name='Batch Normalization')\n",
    "\n",
    "hp_init = hp.HParam(name='init', domain=hp.Discrete(init_list), display_name='Weight Initialization')\n",
    "\n",
    "hp_lr = hp.HParam(name='lr', domain=hp.Discrete(lr_list), display_name='Learning Rate')\n",
    "\n",
    "hp_metric = hp.Metric('accuracy', display_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = os.path.join('lab7-logs-hparams', 'hparam_tuning')\n",
    "root_logdir_writer = tf.summary_create_file_writer(logs_dirs)\n",
    "with root_logdir_writer.as_default():\n",
    "    hp.hparams_config(hparams=[hp_ia, hp_bn, hp_init, hp_lr], metrics=[hp_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = tfds.load('cifar10', split='train[:10%]')\n",
    "\n",
    "train_data_noaug, info = tfds.load('cifar10', split='train[10%:100%]', with_info=True)\n",
    "\n",
    "test_data = tfds.load('cifar10', split=tfds.Split.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_noaug = train_data_noaug.shuffle(train_num)\n",
    "\n",
    "train_data_noaug = train_data_noaug.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_data_noaug = train_data_noaug.batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aug = train_data_aug.shuffle(train_num)\n",
    "\n",
    "train_data_aug = train_data_aug.map(map_func=parse_aug_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_data_aug = train_data_aug.batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = valid_data.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "valid_data = valid_data.batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir, hparams):\n",
    "        super(HyperparameterCallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.hpararms = hparams\n",
    "        self.best_accuracy = 0\n",
    "        self.writer = None\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        \n",
    "        with self.writer.as_default():\n",
    "            hp.hparams(self.hparams)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_accuracy = logs.get('val_categorical_accuracy')\n",
    "        if current_accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = current_accuracy\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('accuracy', self.best_accuracy, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(logs_dir, hparams):\n",
    "    if hparams[hp_init] == 'glorot_normal':\n",
    "        init = initializers.glorot_normal()\n",
    "    elif hparams[hp_init] == 'he_normal':\n",
    "        init = initializers.he_normal()\n",
    "    else:\n",
    "        init = initializers.RandomNormal(0. 0.01)\n",
    "        \n",
    "    inputs = keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(64, (3, 3), kernel_initializer=init)(inputs)\n",
    "    \n",
    "    if hparams[hp_bn]: x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), kernel_initializer=init)\n",
    "        \n",
    "    if hparams[bp_bn] x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(256, (3, 3), kernel_initializer=init)(x)\n",
    "    \n",
    "    if hparams[hp_bn]: x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), kernel_initializer=init)(x)\n",
    "    \n",
    "    if hparams[hp_bn]: x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), kernel_initializer=init)(x)\n",
    "    \n",
    "    if hparams[hp_bn]: x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, (3, 3), kernel_initializer=init)(x)\n",
    "    \n",
    "    if hparams[hp_bn]: x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='model')\n",
    "    \n",
    "    model_tb = keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False)\n",
    "    \n",
    "    model_mckp = keras.callbacks.ModelCheckpoint(log_dir + '/best-model.h5', monitor='val_categorical_accuracy', \n",
    "                                                 save_best_only=True, mode='max')\n",
    "\n",
    "    model_els = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=30, mode='max')\n",
    "    \n",
    "    model_hparam = HpyerparameterCallback(log_dir + 'validation', hparams)\n",
    "    \n",
    "    model.compile(keras.optimizers.Adam(hparams[hp_lr]), loss=keras.losses.CategoricalCrossentropy(), \n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    if hparams[hp_ia]:\n",
    "        history = model.fit(train_data_aug, epochs=100, validation_data=valid_data, \n",
    "                            callbacks=[model_tb, model_mckp, model_els, model_hparam])\n",
    "    else:\n",
    "        history = model.fit(train_data_noaug, epochs=100, validation_data=valid_data, \n",
    "                            callbacks=[model_tb, model_mckp, model_els, model_hparam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 1\n",
    "\n",
    "logs_dir = os.path.join('lab7-logs-hparams', 'run-{}')\n",
    "for ia in ia_list:\n",
    "    for bn in bn_list:\n",
    "        for init in init_list:\n",
    "            for lr in lr_list:\n",
    "                print('--- Running training session {}'.format(session_id))\n",
    "                hparams = {hp_ia: ia, hp_bn: bn, hp_init: init, hp_lr: lr}\n",
    "                train_test_model(logs_dir.format(session_id), hparams)\n",
    "                session_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
